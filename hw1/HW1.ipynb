{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook, tqdm, trange\n",
    "from collections import defaultdict\n",
    "import load_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, InputLayer\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Lambda\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family = \"serif\")\n",
    "plt.rc('xtick', labelsize='x-small')\n",
    "plt.rc('ytick', labelsize='x-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experts = [\"Ant-v2\", \"HalfCheetah-v2\", \"Hopper-v2\", \"Humanoid-v2\", \"Reacher-v2\", \"Walker2d-v2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating expert data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExpert(expert, rollouts) :\n",
    "    process = subprocess.Popen(\"python run_expert.py experts/{0}.pkl {1} --num_rollouts {2}\".format(expert, expert, rollouts), shell = True)\n",
    "    process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = 50\n",
    "for expert in experts :\n",
    "    runExpert(expert, rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting observations, actions from expert data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpertData(expert) :\n",
    "    with open(\"expert_data/{}.pkl\".format(expert), \"rb\") as f :\n",
    "        data = pickle.load(f)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations and Actions :\n",
    "Observation is an array of shape (m, dim_obs) where m are the number of observations and dim_obs is the dimension of the observation array\n",
    "Action is an array of shape (m, 1, dim_act)\n",
    "\n",
    "For the Ant-v2 case, dim_obs = 111 and dim_act = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavior Cloning :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(expert) :\n",
    "    expert_data = getExpertData(expert)\n",
    "    observations, actions, returns = expert_data.values()\n",
    "    #Reshaping action to 2d\n",
    "    actions = actions.reshape((actions.shape[0], actions.shape[-1]))\n",
    "    #Train-validation split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(observations, actions, test_size = 0.1, random_state = 1)\n",
    "    \n",
    "    return(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behaviorCloning(observations, actions, lr = 0.001, epochs = 70, batch_size = 128, verbose = 0) :\n",
    "    #Getting shapes which are used in input and output layers of the network\n",
    "    obs_shape = observations.shape\n",
    "    act_shape = actions.shape\n",
    "    obs_mean = np.mean(observations, axis = 0)\n",
    "    obs_std = np.std(observations, axis = 0) + 1e-9\n",
    "    #inp_obs = (observations - obs_mean) / obs_std \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape = obs_shape[1:]))\n",
    "    #model.add(Lambda(lambda x : (x - obs_mean) / obs_std, input_shape = obs_shape[1:]))\n",
    "    model.add(Dense(units = 64, activation = \"relu\"))\n",
    "    model.add(Dense(units = 64, activation = \"relu\"))\n",
    "    model.add(Dense(units = act_shape[-1]))\n",
    "    \n",
    "    optimizer = Adam(lr)\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = \"mse\", metrics = [\"mse\"])\n",
    "    \n",
    "    model.fit(x = observations, y = actions, epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReturns(expert, model, rollouts) :\n",
    "    model_returns = []\n",
    "    env = gym.make(expert)\n",
    "    max_steps = env.spec.timestep_limit\n",
    "    for i in tqdm_notebook(range(rollouts), desc = \"Rollout\"):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        totalr = 0.\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            obs = obs.reshape((1, obs.shape[0]))\n",
    "            action = model.predict(obs)\n",
    "            action = action.reshape((action.shape[0], 1, action.shape[1]))\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "            if steps >= max_steps:\n",
    "                    break\n",
    "        model_returns.append(totalr)\n",
    "    return(model_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "rollouts = 50\n",
    "lrs = [10 ** i for i in range(-6, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rewards = defaultdict(list)\n",
    "for lr in lrs :\n",
    "    print(\"Learning Rate : {}\".format(lr))\n",
    "    for expert in tqdm_notebook(experts, desc = \"Expert\") :\n",
    "        expert_data = getExpertData(expert)\n",
    "        observations, actions, expert_returns = expert_data[\"observations\"], expert_data[\"actions\"], expert_data[\"returns\"]\n",
    "        actions = np.squeeze(actions)\n",
    "        model = behaviorCloning(observations, actions, lr = lr, epochs = epochs)\n",
    "        model_returns = getReturns(expert, model, rollouts)\n",
    "        lr_rewards[expert].append(model_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (4, 3))\n",
    "for expert in experts :\n",
    "    rewards = lr_rewards[expert]\n",
    "    mean_reward = np.mean(np.array(rewards), axis = 1)\n",
    "    stderr_reward = sem(np.array(rewards), axis = 1)\n",
    "    ax.errorbar(np.log10(lrs), mean_reward, fmt = \"\", yerr = stderr_reward, capsize = 2, alpha = 0.7, linewidth = 0.7, elinewidth = 0.7, label = expert)\n",
    "ax.set_xlabel(\"Log Learning Rate\")\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_xticks(np.log10(lrs))\n",
    "#ax.set_xticklabels([\"-6\", \"-5\", \"-4\", \"-3\", \"-2\", \"-1\", \"1e-0\"])\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "fig.savefig(\"learningrate.png\", bbox_inches = \"tight\", dpi = 300, bbox_extra_artists=(legend,))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate chosen to be 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = np.arange(10, 101, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_rewards = defaultdict(list)\n",
    "for epochs in epochs_list :\n",
    "    print(\"Epochs : {}\".format(epochs))\n",
    "    for expert in tqdm_notebook(experts, desc = \"Expert\") :\n",
    "        expert_data = getExpertData(expert)\n",
    "        observations, actions, expert_returns = expert_data[\"observations\"], expert_data[\"actions\"], expert_data[\"returns\"]\n",
    "        actions = np.squeeze(actions)\n",
    "        model = behaviorCloning(observations, actions, epochs = epochs)\n",
    "        model_returns = getReturns(expert, model, rollouts)\n",
    "        epochs_rewards[expert].append(model_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (4, 3))\n",
    "for expert in experts :\n",
    "    rewards = epochs_rewards[expert]\n",
    "    mean_reward = np.mean(np.array(rewards), axis = 1)\n",
    "    stderr_reward = sem(np.array(rewards), axis = 1)\n",
    "    ax.errorbar(epochs_list, mean_reward, fmt = \"\", yerr = stderr_reward, capsize = 2, alpha = 0.7, linewidth = 0.7, elinewidth = 0.7, label = expert)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_xticks(epochs_list)\n",
    "#ax.set_xticklabels([\"-6\", \"-5\", \"-4\", \"-3\", \"-2\", \"-1\", \"1e-0\"])\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "fig.savefig(\"epochs.png\", bbox_inches = \"tight\", dpi = 300, bbox_extra_artists=(legend,))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for expert in experts :\n",
    "    print(\"Expert : {}\".format(expert))\n",
    "    expert_data = getExpertData(expert)\n",
    "    observations, actions, expert_returns = expert_data[\"observations\"], expert_data[\"actions\"], expert_data[\"returns\"]\n",
    "    actions = np.squeeze(actions)\n",
    "    model = behaviorCloning(observations, actions)\n",
    "    model_returns = getReturns(expert, model, rollouts)\n",
    "    results.append([expert, np.mean(expert_returns), np.mean(model_returns), np.std(expert_returns), np.std(model_returns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns = [\"Expert\", \"Mean Expert Reward\", \"Mean Model Reward\", \"Std Expert Reward\", \"Std Model Reward\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"expert_data/behaviorcloning_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAgger Algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAgger(expert, observations, actions, parameters, iterations) :\n",
    "    obs_dataset = []\n",
    "    act_dataset = []\n",
    "    rewards = []\n",
    "    lr, epochs, batch_size = parameters[\"lr\"], parameters[\"epochs\"], parameters[\"batch_size\"]\n",
    "    train_observations, train_actions = observations, actions\n",
    "    for i in tqdm_notebook(range(iterations), desc = \"DAgger Iteration\") :\n",
    "        #Train model on training set D\n",
    "        model = behaviorCloning(train_observations, train_actions, lr, epochs, batch_size)\n",
    "        env = gym.make(expert)\n",
    "        max_steps = env.spec.timestep_limit\n",
    "        \n",
    "        #Using trained model to get mean rewards\n",
    "        rollouts = 50\n",
    "        returns = []\n",
    "        for j in range(rollouts):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                obs = obs.reshape((1, obs.shape[0]))\n",
    "                model_action = model.predict(obs)\n",
    "                model_action = model_action.reshape((model_action.shape[0], 1, model_action.shape[1]))\n",
    "                obs, r, done, _ = env.step(model_action)\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "        rewards.append(returns)\n",
    "        \n",
    "        \n",
    "        #Start with initial observation and run model to get [o1, o2, o3]\n",
    "        new_observations = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            new_observations.append(obs)\n",
    "            obs = obs.reshape((1, obs.shape[0]))\n",
    "            model_action = model.predict(obs)\n",
    "            model_action = model_action.reshape((model_action.shape[0], 1, model_action.shape[1]))\n",
    "            obs, r, done, _ = env.step(model_action)\n",
    "            steps += 1\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "                    \n",
    "        #Keeping track of rewards\n",
    "        #rewards.append(totalr)\n",
    "        \n",
    "        #Use these observations as input to expert and get expert actions\n",
    "        with tf.Session() :\n",
    "            env = gym.make(expert)\n",
    "            policy_fn = load_policy.load_policy(\"experts/{}.pkl\".format(expert))\n",
    "            new_actions = []\n",
    "\n",
    "            for nobs in new_observations :\n",
    "                expert_action = policy_fn(nobs[None,:])\n",
    "                new_actions.append(expert_action)\n",
    "            \n",
    "        #Get labeled set D_exp = [(o1, a1), (o2, a2), ...]\n",
    "        #Aggregate this to training set (D = D + D_exp) and retrain model\n",
    "        obs_dataset.append(train_observations)\n",
    "        act_dataset.append(train_actions)\n",
    "        train_observations = np.concatenate((train_observations, np.array(new_observations)))\n",
    "        train_actions = np.concatenate((train_actions, np.squeeze(np.array(new_actions))))\n",
    "        \n",
    "    return(obs_dataset, act_dataset, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dataset = {}\n",
    "act_dataset = {}\n",
    "rewards_dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = [\"Hopper-v2\"]\n",
    "for expert in exp :\n",
    "    print(\"Expert : {}\".format(expert))\n",
    "    expert_data = getExpertData(expert)\n",
    "    observations, actions, expert_returns = expert_data[\"observations\"], expert_data[\"actions\"], expert_data[\"returns\"]\n",
    "    actions = np.squeeze(actions)\n",
    "    obs_dataset[expert], act_dataset[expert], rewards_dataset[expert] = DAgger(expert, observations, actions, {\"lr\" : 0.001, \"epochs\" : 70, \"batch_size\" : 128}, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = \"Hopper-v2\"\n",
    "mean_rewards = [np.mean(rollout_rewards) for rollout_rewards in rewards_dataset[expert]][:20]\n",
    "stderr_rewards = [sem(rollout_rewards) for rollout_rewards in rewards_dataset[expert]][:20]\n",
    "expert_performance = df[df[\"Expert\"] == expert][\"Mean Expert Reward\"].values[0]\n",
    "bc_performance = df[df[\"Expert\"] == expert][\"Mean Model Reward\"].values[0]\n",
    "x = np.arange(1, 21, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (4, 3))\n",
    "ax.errorbar(x, mean_rewards, fmt = \"\", yerr = stderr_rewards, ecolor = \"black\", capsize = 2, alpha = 0.7, linewidth = 0.7, elinewidth = 0.7, color = \"blue\", label = \"DAgger\")\n",
    "#ax.errorbar(x, mean_rewards, fmt = \"o\", yerr = stderr_rewards, ecolor = \"black\", mfc = \"None\", markersize = 4, mec = \"blue\", capsize = 2, alpha = 0.7, mew = 0.7, elinewidth = 0.7)\n",
    "ax.plot([1, 21], [expert_performance, expert_performance], linestyle = \"--\", color = \"green\", linewidth = 0.7, label = \"Expert\")\n",
    "ax.plot([1, 21], [bc_performance, bc_performance], linestyle = \"--\", color = \"red\", linewidth = 0.7, label = \"Behavior Cloning\")\n",
    "ax.set_xlabel(\"DAgger Iterations\")\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_xticks(np.arange(0, 21, 2))\n",
    "ax.legend()\n",
    "fig.savefig(\"Hopper-v2-dagger20.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert : Humanoid-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3e1b6e7d9c4e119628a4a16ce77cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='DAgger Iteration', max=50, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n",
      "obs (1, 376) (1, 376)\n"
     ]
    }
   ],
   "source": [
    "exp = [\"Humanoid-v2\"]\n",
    "for expert in exp :\n",
    "    print(\"Expert : {}\".format(expert))\n",
    "    expert_data = getExpertData(expert)\n",
    "    observations, actions, expert_returns = expert_data[\"observations\"], expert_data[\"actions\"], expert_data[\"returns\"]\n",
    "    actions = np.squeeze(actions)\n",
    "    obs_dataset[expert], act_dataset[expert], rewards_dataset[expert] = DAgger(expert, observations, actions, {\"lr\" : 0.001, \"epochs\" : 70, \"batch_size\" : 128}, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"expert_data/behaviorcloning_results.csv\", index_col = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = \"Humanoid-v2\"\n",
    "mean_rewards = [np.mean(rollout_rewards) for rollout_rewards in rewards_dataset[expert]]\n",
    "stderr_rewards = [sem(rollout_rewards) for rollout_rewards in rewards_dataset[expert]]\n",
    "expert_performance = df[df[\"Expert\"] == expert][\"Mean Expert Reward\"].values[0]\n",
    "bc_performance = df[df[\"Expert\"] == expert][\"Mean Model Reward\"].values[0]\n",
    "x = np.arange(1, 51, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (4, 3))\n",
    "ax.errorbar(x, mean_rewards, fmt = \"\", yerr = stderr_rewards, ecolor = \"black\", capsize = 2, alpha = 0.7, linewidth = 0.7, elinewidth = 0.7, color = \"blue\", label = \"DAgger\")\n",
    "#ax.errorbar(x, mean_rewards, fmt = \"o\", yerr = stderr_rewards, ecolor = \"black\", mfc = \"None\", markersize = 4, mec = \"blue\", capsize = 2, alpha = 0.7, mew = 0.7, elinewidth = 0.7)\n",
    "ax.plot([1, 51], [expert_performance, expert_performance], linestyle = \"--\", color = \"green\", linewidth = 0.7, label = \"Expert\")\n",
    "ax.plot([1, 51], [bc_performance, bc_performance], linestyle = \"--\", color = \"red\", linewidth = 0.7, label = \"Behavior Cloning\")\n",
    "ax.set_xlabel(\"DAgger Iterations\")\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_xticks(np.arange(0, 53, 4))\n",
    "ax.legend()\n",
    "fig.savefig(\"Humanoid-v2-dagger50.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = \"Humanoid-v2\"\n",
    "mean_rewards = [np.mean(rollout_rewards) for rollout_rewards in rewards_dataset[expert]][:20]\n",
    "stderr_rewards = [sem(rollout_rewards) for rollout_rewards in rewards_dataset[expert]][:20]\n",
    "expert_performance = df[df[\"Expert\"] == expert][\"Mean Expert Reward\"].values[0]\n",
    "bc_performance = df[df[\"Expert\"] == expert][\"Mean Model Reward\"].values[0]\n",
    "x = np.arange(1, 21, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (4, 3))\n",
    "ax.errorbar(x, mean_rewards, fmt = \"\", yerr = stderr_rewards, ecolor = \"black\", capsize = 2, alpha = 0.7, linewidth = 0.7, elinewidth = 0.7, color = \"blue\", label = \"DAgger\")\n",
    "#ax.errorbar(x, mean_rewards, fmt = \"o\", yerr = stderr_rewards, ecolor = \"black\", mfc = \"None\", markersize = 4, mec = \"blue\", capsize = 2, alpha = 0.7, mew = 0.7, elinewidth = 0.7)\n",
    "ax.plot([1, 21], [expert_performance, expert_performance], linestyle = \"--\", color = \"green\", linewidth = 0.7, label = \"Expert\")\n",
    "ax.plot([1, 21], [bc_performance, bc_performance], linestyle = \"--\", color = \"red\", linewidth = 0.7, label = \"Behavior Cloning\")\n",
    "ax.set_xlabel(\"DAgger Iterations\")\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_xticks(np.arange(0, 21, 2))\n",
    "ax.legend()\n",
    "fig.savefig(\"Humanoid-v2-dagger20.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
